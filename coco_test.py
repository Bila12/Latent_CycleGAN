# -*- coding: utf-8 -*-
"""Copie de clip_prefix_captioning_inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19SI91A53cZhL0dvTOAwSSNQbE7rFxCvj

# Inference notenook for [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption/)

Disclaimer: the authors do not own any rights for the code or data.
"""


#@title Drive Downloader



import clip
import os
from torch import nn
import numpy as np
import torch
import torch.nn.functional as nnf
import sys
from typing import Tuple, List, Union, Optional
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import skimage.io as io
import PIL.Image
from clip.clip import tokenize
from Cycle_consistency_training import Generator
import os
import json
from transformers import ViTFeatureExtractor, ViTModel

from predict import *

#@title Choose pretrained model - COCO or Coneptual captions

device = torch.device('cuda:0')

clip_model, preprocess = clip.load("ViT-B/32", device=device, jit=False)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
generator_text = Generator(512,512).to(device)
generator_text.load_state_dict(torch.load("./coco_train/generator_text-019_id12_onlytrain.pt"))
#generator_im = Generator(512,512).to(device)
#generator_im.load_state_dict(torch.load("./coco_train/generator_im-019_id12.pt"))


#@title Load model weights


prefix_length = 10
clip_length = 10

from train import MappingType
from train import ClipCaptionModel

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')
vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(device)

'''
model_text = ClipCaptionModel(prefix_length, clip_length=clip_length, mapping_type=MappingType.Transformer, prefix_size=512)

model_text_path = "./checkpoints/coco_prefix-018_image_onlytrain.pt"

model_text.load_state_dict(torch.load(model_text_path, map_location=CPU)) 

model_text = model_text.eval() 

model_text = model_text.to(device)



model_im = ClipCaptionModel(prefix_length, clip_length=clip_length, mapping_type=MappingType.Transformer, prefix_size=512)
model_im_path = "./coco_train/coco_prefix-009_image.pt"

model_im.load_state_dict(torch.load(model_im_path, map_location=CPU)) 

model_im = model_im.eval() 

model_im = model_im.to(device)
'''
model_text = ClipCaptionModel(prefix_length, clip_length=clip_length, mapping_type=MappingType.Transformer, prefix_size=768)
model_text_path = "./checkpoints/coco_prefix-011_ViT_decoder_prefix10_transformer_onlytrain.pt"

model_text.load_state_dict(torch.load(model_text_path, map_location=CPU)) 

model_text = model_text.eval() 

model_text = model_text.to(device)


results = []

#@title Or download random samples form COCO test set (Karpathy et al. split)
for UPLOADED_FILE in os.listdir("/mnt/SSD/datasets/coco/data/val2014/"):

    
    image = io.imread("/mnt/SSD/datasets/coco/data/val2014/"+UPLOADED_FILE)
    if image.size < 501000:
        img_id = UPLOADED_FILE[-16:-4]
        results.append({"image_id": img_id, "caption": " ",})
        continue
    inputs = feature_extractor(images=image, return_tensors="pt").to(device)   
    #image = io.imread('./Images/cal.jpg')
    pil_image = PIL.Image.fromarray(image)
    #pil_img = Image(filename=UPLOADED_FILE)
    
    
    image = preprocess(pil_image).unsqueeze(0).to(device)
    
    with torch.no_grad():
    # if type(model) is ClipCaptionE2E:
    #     prefix_embed = model.forward_image(image)
    # else:
    
        #prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)
        #prefix
        #prefix_translated = generator_text(prefix)
        #prefix_embed_trans_text = model_text.clip_project(prefix_translated).reshape(1, prefix_length, -1)
        #prefix_embed_im = model_text.clip_project(prefix).reshape(1, prefix_length, -1)
        #print(model==model_im)
        
        vit_outputs = vit_model(**inputs)
        last_hidden_states = vit_outputs.last_hidden_state
        prefix_im = last_hidden_states[0][0].unsqueeze(dim=0)
        prefix_text = model_text.clip_project(prefix_im).reshape(1, prefix_length, -1)
        #generated_im_prefix_text = generate_beam(model_text, tokenizer, embed=prefix_text)[0]
        img_id = UPLOADED_FILE[-16:-4]
        #(img_id)
    
        
        generated_text = generate_beam(model_text, tokenizer, embed=prefix_text)[0]
        results.append({"image_id": img_id, "caption": generated_text,})
        # Serializing json 
        
json_object = json.dumps(results)
# Writing to sample.json
with open("captions_val2014_Vit_prefix_caption_im_results_onlytrain.json", "w") as outfile:
    outfile.write(json_object)
    
        