# -*- coding: utf-8 -*-
"""Copie de clip_prefix_captioning_inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19SI91A53cZhL0dvTOAwSSNQbE7rFxCvj

# Inference notenook for [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption/)

Disclaimer: the authors do not own any rights for the code or data.
"""


#@title Drive Downloader



import clip
import os
from torch import nn
import numpy as np
import torch
import torch.nn.functional as nnf
import sys
from typing import Tuple, List, Union, Optional
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import skimage.io as io
import PIL.Image
from clip.clip import tokenize
from Cycle_consistency_training import Generator



from predict import *

#@title Choose pretrained model - COCO or Coneptual captions

device = torch.device('cuda:0')

clip_model, preprocess = clip.load("ViT-B/32", device=device, jit=False)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
generator_text = Generator(512,512).to(device)
generator_text.load_state_dict(torch.load("./coco_train/generator_text-019_id12.pt"))
generator_im = Generator(512,512).to(device)
generator_im.load_state_dict(torch.load("./coco_train/generator_im-019_id12.pt"))


#@title Load model weights


prefix_length = 40
clip_length = 40

from train import MappingType
from train import ClipCaptionModel
model_text = ClipCaptionModel(prefix_length, clip_length=clip_length, mapping_type=MappingType.Transformer, prefix_size=512)
model_text_path = "./coco_train/coco_prefix-009.pt"

model_text.load_state_dict(torch.load(model_text_path, map_location=CPU)) 

model_text = model_text.eval() 

model_text = model_text.to(device)


model_im = ClipCaptionModel(prefix_length, clip_length=clip_length, mapping_type=MappingType.Transformer, prefix_size=512)
model_im_path = "./coco_train/coco_prefix-009_image.pt"

model_im.load_state_dict(torch.load(model_im_path, map_location=CPU)) 

model_im = model_im.eval() 

model_im = model_im.to(device)

#@title Or download random samples form COCO test set (Karpathy et al. split)

IMAGE_NAME = '299391'  # @param ['562207', '579664', '060623', '165547', '334321', '483108', '386164', '354533']

name_ = "COCO_test2014_000000" + IMAGE_NAME + ".jpg"
images_path = os.path.join('/mnt/SSD/datasets/coco/data/', "test2014")
UPLOADED_FILE = os.path.join(images_path, name_)



#@title Inference
use_beam_search = True #@param {type:"boolean"}  

image = io.imread(UPLOADED_FILE)
#image = io.imread('./Images/cal.jpg')
pil_image = PIL.Image.fromarray(image)
#pil_img = Image(filename=UPLOADED_FILE)


image = preprocess(pil_image).unsqueeze(0).to(device)
with torch.no_grad():
    # if type(model) is ClipCaptionE2E:
    #     prefix_embed = model.forward_image(image)
    # else:
    '''
    capts = ["A male kennel in orange and black mourning penguins","A black and white bird wearing a long sleeve shirt while standing in the water."]
    
    for capt in capts:
        capt_tok = tokenize(capt).to(device)
        prefix = clip_model.encode_text(capt_tok).to(device, dtype=torch.float32)
        prefix_embed_text = model_text.clip_project(prefix).reshape(1, prefix_length, -1)
        prefix_embed_im = model_im.clip_project(prefix).reshape(1, prefix_length, -1)
        prefix_translated = generator_im(prefix)
        prefix_embed_trans_text = model_text.clip_project(prefix_translated).reshape(1, prefix_length, -1)
        prefix_embed_trans_im = model_im.clip_project(prefix_translated).reshape(1, prefix_length, -1)
        if use_beam_search:
            generated_text_prefix = generate_beam(model_text, tokenizer, embed=prefix_embed_text)[0]
            generated_im_prefix = generate_beam(model_im, tokenizer, embed=prefix_embed_im)[0]
            generated_trans_prefix = generate_beam(model_im, tokenizer, embed=prefix_embed_trans_im)[0]
            
        else:
            generated_text_prefix = generate_beam(model_text, tokenizer, embed=prefix_embed)
            generated_im_prefix = generate_beam(model_im, tokenizer, embed=prefix_embed)
            generated_trans_prefix = generate_beam(model_im, tokenizer, embed=prefix_embed_trans)
    
    
        print('\n')
        print(generated_text_prefix)
        print(generated_im_prefix)
        print(generated_trans_prefix)
        
    '''
    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)
    prefix_embed_text = model_text.clip_project(prefix).reshape(1, prefix_length, -1)
    prefix_embed_im = model_im.clip_project(prefix).reshape(1, prefix_length, -1)
    prefix_translated = generator_text(prefix)
    prefix_embed_trans_text = model_text.clip_project(prefix_translated).reshape(1, prefix_length, -1)
    prefix_embed_trans_im = model_im.clip_project(prefix_translated).reshape(1, prefix_length, -1)
    #print(model==model_im)
    if use_beam_search:
        generated_text_prefix = generate_beam(model_text, tokenizer, embed=prefix_embed_text)[0]
        generated_im_prefix = generate_beam(model_im, tokenizer, embed=prefix_embed_im)[0]
        generated_translated_prefix_text = generate_beam(model_text, tokenizer, embed=prefix_embed_trans_text)[0]
        generated_translated_prefix_im = generate_beam(model_im, tokenizer, embed=prefix_embed_trans_im)[0]
    else:
        generated_text_prefix = generate_beam(model_text, tokenizer, embed=prefix_embed)
        generated_im_prefix = generate_beam(model_im, tokenizer, embed=prefix_embed)
        generated_translated_prefix_text = generate_beam(model_text, tokenizer, embed=prefix_embed_trans)
        generated_translated_prefix_im = generate_beam(model_im, tokenizer, embed=prefix_embed_trans)
    
    
print('\n')
print(generated_im_prefix)
print(generated_text_prefix)
print(generated_translated_prefix_text)
print(generated_translated_prefix_im)
        