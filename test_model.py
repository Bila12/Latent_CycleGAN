# -*- coding: utf-8 -*-
"""Copie de clip_prefix_captioning_inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19SI91A53cZhL0dvTOAwSSNQbE7rFxCvj

# Inference notenook for [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption/)

Disclaimer: the authors do not own any rights for the code or data.
"""


#@title Drive Downloader



import clip
import os
from torch import nn
import numpy as np
import torch
import torch.nn.functional as nnf
import sys
from typing import Tuple, List, Union, Optional
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import skimage.io as io
import PIL.Image
from clip.clip import tokenize



from predict import *

#@title Choose pretrained model - COCO or Coneptual captions

device = torch.device('cuda:0')

clip_model, preprocess = clip.load("ViT-B/32", device=device, jit=False)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

#@title Load model weights


prefix_length = 40
clip_length = 40

from train import MappingType
from train import ClipCaptionModel
model = ClipCaptionModel(prefix_length, clip_length=clip_length, mapping_type=MappingType.Transformer, prefix_size=512)
model_path = "./coco_train/coco_prefix-000_simple_shape.pt"

model.load_state_dict(torch.load(model_path, map_location=CPU)) 

model = model.eval() 

model = model.to(device)

#@title Or download random samples form COCO test set (Karpathy et al. split)

IMAGE_NAME = '354533'  # @param ['562207', '579664', '060623', '165547', '334321', '483108', '386164', '354533']

name_ = "COCO_val2014_000000" + IMAGE_NAME + ".jpg"
images_path = os.path.join('./', "Images")
UPLOADED_FILE = os.path.join(images_path, name_)



#@title Inference
use_beam_search = True #@param {type:"boolean"}  

image = io.imread(UPLOADED_FILE)
pil_image = PIL.Image.fromarray(image)
#pil_img = Image(filename=UPLOADED_FILE)


image = preprocess(pil_image).unsqueeze(0).to(device)
with torch.no_grad():
    # if type(model) is ClipCaptionE2E:
    #     prefix_embed = model.forward_image(image)
    # else:
    
    capts = ["A kind of dark slate grey average sized oval shaped structure, located in the bottom right, and is rotated 245 degrees.","A little arrow-shaped polygon in turquoise color. It is at the upper left and is pointing towards the west.","This is a midnight blue big four-sided shape, at the top right. It is pointing north-west.","The image is a teal medium sized trapezoidal shape. It is in the middle and is pointing to the south.","The image contains a light slate grey tiny deformed square shape, located in the center left. It is pointing south-west.", "A motorcycle parked in the desert"]
    
    for capt in capts:
        capt_tok = tokenize(capt).to(device)
        prefix = clip_model.encode_text(capt_tok).to(device, dtype=torch.float32)
        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)
        if use_beam_search:
            generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]
        else:
            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)
    
    
        print('\n')
        print(generated_text_prefix)
        '''
    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)
    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)
    if use_beam_search:
        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]
    else:
            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)
    
    
print('\n')
print(generated_text_prefix)
        '''